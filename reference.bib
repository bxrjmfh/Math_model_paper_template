
@book{1,
	title = {供应链物流管理  原书第3版},
	isbn = {978-7-111-28895-4},
	url = {http://book.ucdrs.superlib.net/views/specific/2929/bookDetail.jsp?dxNumber=000006817531&d=312FF8FC051CC45D4A9329C1244C99F4&fenlei=06030703},
	abstract = {本书不仅涵盖了物流/供应链管理方面的发展进程和基本原理，而且为我们展示了未来商业物流发展的愿景与供应链管理及其在增强企业竞争力方面所起到的作用，还提供了一个新的物流领域的研究框架，立足于从当代商业的角度考察物流的运作方法，而且特别关注物流在全球竞争战略中日益提高的重要性，同时，揭示了一体化物流管理所需的行动、商业流程和战略，并使物流整合成为企业战略的一种核心竞争力。},
	pagetotal = {393},
	publisher = {机械工业出版社},
	author = {{（美）唐纳德J.鲍尔索克斯，戴维J.克劳斯，M.比克斯比.库珀编}},
	urldate = {2022-09-10},
	date = {2010-01},
}

@inreference{2,
	title = {层次分析法},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://zh.wikipedia.org/w/index.php?title=%E5%B1%A4%E7%B4%9A%E5%88%86%E6%9E%90%E6%B3%95&oldid=73182724},
	abstract = {层次分析法（英语：Analytic Hierarchy Process, {AHP}）为 1971 年Thomas L. Saaty （匹兹堡大学教授）所发展出来，主要应用在不确定情况下及具有多数个评估准则的决策问题上。
层次分析法发展的目的是将复杂的问题系统化，由不同层面给予层级分解，并透过量化的运算，找到脉络后加以综合评估 。
1971年，Saaty 替美国国防部从事应变计划问题（Contingency Planning Problem）的研究，并于 1972 年在美国国家科学基金会资助下，进行各产业电力合理分配的研究。1972 年 7 月，Saaty 在开罗替埃及政府从事‘无和平、无战争’（No Peace, No War）对埃及经济、政治状况的影响研究时，开始将有关的判断尺度化。1973 年，Saaty 将 {AHP} 法应用在苏丹运输研究后，整个理论才趋成熟；其后在 1974 年至 1978 年间，经不断应用修正及证明后，使得整个理论更臻完备。1980 年，Saaty 遂将此一理论整理成专书问世，随后在 1982 年至 1987 年间，相继出版有关 {AHP} 理论的专著共三册。{AHP} 发展以来，在国际期刊发表的相关论文不断的出现，而且应用的范围也相当的广泛。},
	booktitle = {维基百科，自由的百科全书},
	urldate = {2022-09-10},
	date = {2022-08-12},
	langid = {pinyin},
	note = {Page Version {ID}: 73182724},
	file = {Snapshot:D\:\\WEATH\\storage\\ETVWZDHM\\層級分析法.html:text/html},
}

@inreference{3,
	title = {Entropy},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Entropy&oldid=1109401529},
	abstract = {Entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.The thermodynamic concept was referred to by Scottish scientist and engineer Macquorn Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.A consequence of entropy is that certain processes are irreversible or impossible, aside from the requirement of not violating the conservation of energy, the latter being expressed in the first law of thermodynamics. Entropy is central to the second law of thermodynamics, which states that the entropy of isolated systems left to spontaneous evolution cannot decrease with time, as they always arrive at a state of thermodynamic equilibrium, where the entropy is highest.
Austrian physicist Ludwig Boltzmann explained entropy as the measure of the number of possible microscopic arrangements or states of individual atoms and molecules of a system that comply with the macroscopic condition of the system. He thereby introduced the concept of statistical disorder and probability distributions into a new field of thermodynamics, called statistical mechanics, and found the link between the microscopic interactions, which fluctuate about an average configuration, to the macroscopically observable behavior, in form of a simple logarithmic law, with a proportionality constant, the Boltzmann constant, that has become one of the defining universal constants for the modern International System of Units ({SI}).
In 1948, Bell Labs scientist Claude Shannon developed similar statistical concepts of measuring microscopic uncertainty and multiplicity to the problem of random losses of information in telecommunication signals. Upon John von Neumann's suggestion, Shannon named this entity of missing information in analogous manner to its use in statistical mechanics as entropy, and gave birth to the field of information theory. This description has been identified as a universal definition of the concept of entropy.},
	booktitle = {Wikipedia},
	urldate = {2022-09-10},
	date = {2022-09-09},
	langid = {english},
	note = {Page Version {ID}: 1109401529},
	file = {Snapshot:D\:\\WEATH\\storage\\97VPXW4N\\Entropy.html:text/html},
}